{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, vocab_size, max_len=10):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.encoder = nn.GRU(emb_dim, hidden_dim)\n",
    "        self.decoder = nn.GRU(hidden_dim + emb_dim, hidden_dim)\n",
    "        self.Wb = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def encoding(self, inp):\n",
    "        encoded = []\n",
    "        h_t = self.h_0()\n",
    "        for i, w in enumerate(inp):\n",
    "            x = self.emb(torch.tensor(w).long()).view((1, 1, emb_dim))\n",
    "            output, h_t = self.encoder(x, h_t)\n",
    "            encoded.append(h_t)\n",
    "        return encoded\n",
    "    \n",
    "    def decoding(self, encoded, target):\n",
    "        s_t = self.h_0()\n",
    "        c_t = torch.mean(torch.cat(encoded), dim=0).view((1, 1, hidden_dim))\n",
    "        y_t = self.emb(torch.tensor(w2id['<B>']).long()).view((1, 1, -1))\n",
    "        \n",
    "        losses = []\n",
    "        for wid in target:\n",
    "            x = torch.cat([c_t, y_t], dim=2)\n",
    "            output, s_t = self.decoder(x, s_t)\n",
    "\n",
    "            probs = self.softmax(self.Wb(s_t)[0])\n",
    "            losses.append(probs)\n",
    "            \n",
    "            y_t = self.emb(torch.tensor(wid).long()).view((1, 1, -1))\n",
    "        return losses\n",
    "    \n",
    "    def generating(self, encoded):\n",
    "        s_t = self.h_0()\n",
    "        c_t = torch.mean(torch.cat(encoded), dim=0).view((1, 1, hidden_dim))\n",
    "        y_t = self.emb(torch.tensor(w2id['<B>']).long()).view((1, 1, -1))\n",
    "        \n",
    "        words = []\n",
    "        for wid in range(self.max_len):\n",
    "            x = torch.cat([c_t, y_t], dim=2)\n",
    "            output, s_t = self.decoder(x, s_t)\n",
    "\n",
    "            probs = self.softmax(self.Wb(s_t)[0])\n",
    "            idx = torch.argmax(probs)\n",
    "            y_t = self.emb(idx).view((1, 1, -1))\n",
    "            words.append(idx)\n",
    "        return words\n",
    "    \n",
    "    def forward(self, inp, target=None):\n",
    "        encoded = self.encoding(inp)\n",
    "        \n",
    "        if target:\n",
    "            return self.decoding(encoded, target)\n",
    "        else:\n",
    "            return self.generating(encoded)\n",
    "\n",
    "    def h_0(self):\n",
    "        return torch.zeros((1, 1, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "corpus = ['<B> Eu gosto de você . <E>', '<B> Eu não gosto de você . <E>', '<B> Eu não amo você . <E>']\n",
    "corpus = [w.split() for w in corpus]\n",
    "\n",
    "vocab = []\n",
    "for snt in corpus:\n",
    "    for w in snt:\n",
    "        vocab.append(w)\n",
    "vocab = set(vocab)\n",
    "w2id = { w:i for i, w in enumerate(vocab) }\n",
    "id2w = { i:w for i, w in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (emb): Embedding(9, 5)\n",
      "  (encoder): GRU(5, 10)\n",
      "  (decoder): GRU(15, 10)\n",
      "  (Wb): Linear(in_features=10, out_features=9, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "emb_dim, hidden_dim, vocab_size = 5, 10, len(vocab)\n",
    "model = Seq2Seq(emb_dim, hidden_dim, vocab_size)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "tensor(2.1972, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1913, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1948, grad_fn=<NllLossBackward>)\n",
      "Epoch:  2\n",
      "tensor(2.1818, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1788, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1812, grad_fn=<NllLossBackward>)\n",
      "Epoch:  3\n",
      "tensor(2.1672, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1646, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1634, grad_fn=<NllLossBackward>)\n",
      "Epoch:  4\n",
      "tensor(2.1497, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1474, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1412, grad_fn=<NllLossBackward>)\n",
      "Epoch:  5\n",
      "tensor(2.1281, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1257, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1141, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print('Epoch: ', epoch+1)\n",
    "    for row in corpus:\n",
    "        optimizer.zero_grad()\n",
    "        snt_ids = [w2id[w] for w in row]\n",
    "        output = model(snt_ids, snt_ids)\n",
    "\n",
    "        loss = criterion(torch.cat(output, dim=0), torch.tensor(snt_ids))\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(5), tensor(2), tensor(4), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Eu', 'gosto', '.', '<E>', '<E>', '<E>', '<E>', '<E>', '<E>', '<E>']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for snt_ids in corpus:\n",
    "    print([id2w[int(w)] for w in model(snt_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "emb_dim, hidden_dim, vocab_size = 5, 10, len(vocab)\n",
    "\n",
    "emb = nn.Embedding(vocab_size, emb_dim)\n",
    "encoder = nn.GRU(emb_dim, hidden_dim)\n",
    "decoder = nn.GRU(hidden_dim + emb_dim, hidden_dim)\n",
    "Wb = nn.Linear(hidden_dim, vocab_size)\n",
    "softmax = nn.LogSoftmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "snt = corpus[0]\n",
    "snt_ids = [w2id[w] for w in snt]\n",
    "\n",
    "encoded = []\n",
    "h_t = torch.zeros((1, 1, hidden_dim))\n",
    "for i, snt_id in enumerate(snt_ids):\n",
    "    x = emb(torch.tensor(snt_id).long()).view((1, 1, emb_dim))\n",
    "    \n",
    "    output, h_t = encoder(x, h_t)\n",
    "    encoded.append(h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9836, -2.2431, -1.8425, -2.4867, -1.9412, -2.2557, -2.0177, -2.0143]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(2)\n",
      "tensor([[-2.0522, -2.1550, -2.0337, -2.3604, -1.9261, -2.3631, -1.8979, -1.9611]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(6)\n",
      "tensor([[-1.8888, -2.1833, -1.7217, -2.5176, -2.0078, -2.3238, -2.1533, -2.0528]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(2)\n",
      "tensor([[-2.0029, -2.0828, -2.0165, -2.3504, -1.9493, -2.4000, -1.9807, -1.9594]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(4)\n",
      "tensor([[-2.0358, -2.1827, -1.8251, -2.3776, -1.8816, -2.3632, -2.1038, -2.0081]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "s_t = torch.zeros((1, 1, hidden_dim))\n",
    "c_t = torch.mean(torch.cat(encoded), dim=0).view((1, 1, hidden_dim))\n",
    "y_t = emb(torch.tensor(w2id['<B>']).long()).view((1, 1, -1))\n",
    "for i in range(5):\n",
    "    x = torch.cat([c_t, y_t], dim=2)\n",
    "    output, s_t = decoder(x, s_t)\n",
    "    \n",
    "    word_dist = softmax(Wb(s_t)[0])\n",
    "    print(word_dist)\n",
    "    idx = torch.argmax(word_dist)\n",
    "    y_t = emb(idx).view((1, 1, -1))\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1290, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dist[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
